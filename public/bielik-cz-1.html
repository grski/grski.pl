
<!DOCTYPE html>
<html lang="pl">

<head>
    <title>Polacy nie gęsi swój język mają cz. I: Bielik.AI podrywa się do lotu. - Olaf Górski</title>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="Odpalamy pierwszy i największy polski model językowy lokalnie i dokonujemy pierwszego requesta" />

    <meta name="robots" content="index, follow">
    <meta property="og:title" content="Polacy nie gęsi swój język mają cz. I: Bielik.AI podrywa się do lotu.">
    <meta property="og:description" content="Odpalamy pierwszy i największy polski model językowy lokalnie i dokonujemy pierwszego requesta">
    <meta property="og:url" content="https://grski.pl/">
    <meta property="og:site_name" content="The Engineer - Olaf Górski">
    <meta property="og:type" content="website">
    <meta property="article:section" content="">
    <meta property="og:updated_time" content="2025-05-06T00:00:00Z" />

    <link rel="stylesheet" href="https://grski.pl/static/styles/style.min.css" />
    <link rel="stylesheet" href="https://grski.pl/static/styles/highlighting.min.css" />
    <link rel="shortcut icon" type="image/png" href="https://grski.pl/static/favicon.png"/>
    <meta name="theme-color" content="#ffffff">
    
</head>

<body>
<section class="section">
    <div class="container">
        <nav id="nav-main" class="nav">
            <div id="nav-name" class="nav-left">
                <a id="nav-anchor" class="title is-4 nav-item" href="https://grski.pl/">
                    The Engineer by Olaf Górski - on Python & AI
                </a>
            </div>
            <div class="nav-right">
                <nav id="nav-items" class="nav-item level is-mobile">
                    
                    <a class="level-item" aria-label="github" href='https://github.com/grski' target='_blank' rel='noopener'><span class="icon">
                                <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
                                    <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/>
                                </svg></i>
                        </span></a>
                    
                    
                    <a class="level-item" aria-label="linkedin" href='https://www.linkedin.com/in/olafgorski/' target='_blank' rel='noopener'><span class="icon">
                                <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
                                    <path stroke-width="1.8" d="m5.839218,4.101561c0,1.211972 -0.974141,2.194011 -2.176459,2.194011s-2.176459,-0.982039 -2.176459,-2.194011c0,-1.211094 0.974141,-2.194011 2.176459,-2.194011s2.176459,0.982917 2.176459,2.194011zm0.017552,3.94922l-4.388022,0l0,14.04167l4.388022,0l0,-14.04167zm7.005038,0l-4.359939,0l0,14.04167l4.360816,0l0,-7.370999c0,-4.098413 5.291077,-4.433657 5.291077,0l0,7.370999l4.377491,0l0,-8.89101c0,-6.915523 -7.829986,-6.66365 -9.669445,-3.259423l0,-1.891237z"/>
                                </svg></i>
                            </span></a>
                    
                </nav>
            </div>
        </nav>

        <nav class="nav">
            <!-- todo -->
        </nav>
        

    </div>
</section>

    <section class="section">
        <div class="container">
            <article>
                <div class="subtitle tags is-6 is-pulled-right">
             <!--       <a class="subtitle is-6" href="">#html</a> | <a class="subtitle is-6" href="https://themes.gohugo.io//theme/kiss/tags/themes/kiss.j2">#themes</a>-->
                </div>
                <h3 class="subtitle is-6 date">2025-05-06</h3>
                <h1 class="title"><a href="https://grski.pl/">Polacy nie gęsi swój język mają cz. I: Bielik.AI podrywa się do lotu.</a></h1>
                <div class="content">
                    <p>Tak jak w tytule - Polacy nie gęsi swój język mają, tak samo jak i swój model językowy. Bielik.ai to projekt tworzony przez polską społeczność, który ma na celu stworzenie najlepszego polskiego modela językowego, który doskonale zna niuanse i realia naszego pięknego języka jak i kraju.</p>
<p>Uważam, że to wspaniała inicjatywa, którą warto nagłaśniać, stąd też, w połączeniu z faktem, że mam jeszcze kilka dni wolnego, postanowiłem stworzyć cykl artykułów poświęconych bielikowi i jego wykorzystaniu w praktyce.</p>
<p>Pierwszy z nich będzie o... uruchamianiu. Jak to wyglądało u mnie i jak można to zrobić. Bo nie wiem czy wiecie, LLMy obecnie nie musza być aż tak duże jak to ich nazwa (Larga Language Models) wskazuje!</p>
<p>Większość z nich można odpalić na domowym sprzęcie. Dla przykładu skorzystam mojego Maca - Air z 16GB na pokładzie i M2. Nie jest to sprzęt z górnej półki zdecydowanie. Ciekawe czy damy radę odpalić na nim coś? Jak sądzicie? Zobaczmy.</p>
<p>Zanim to zrobimy jednak trzeba by zadać sobie pytanie w jaki sposób możemy odpalać modele językowe na naszych maszynach.</p>
<h2>Llama.cpp vs vllm</h2>
<p>Obecnie na rynku jest sporo rozwiązań, takie dwa topowe, to moim zdaniem llama.cpp i vllm. Zachęcony obietnicami projektu postanowiłem skorzystać najpierw z vllm by odpalić bielika. Jak poszło?</p>
<h3>vllm</h3>
<p>Vllm obiecał mi, że będzie trendy i cool i takie tam. Co dostarczył. Nie za dużo, przynajmniej jeśli idzie o instalacje.</p>
<div class="codehilite"><pre><span></span><code>brew<span class="w"> </span>install<span class="w"> </span>cmake<span class="w"> </span>pkg-config<span class="w"> </span>sentencepiece<span class="w"> </span>protobuf-c<span class="w"> </span>
pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>pip<span class="w"> </span>wheel<span class="w"> </span>setuptools<span class="w"> </span>
pip<span class="w"> </span>install<span class="w"> </span>--pre<span class="w"> </span><span class="nv">torch</span><span class="o">==</span><span class="m">2</span>.5.1<span class="w"> </span>torchvision<span class="w"> </span>torchaudio
brew<span class="w"> </span>install<span class="w"> </span>ninja<span class="w"> </span>rust<span class="w"> </span>
<span class="nv">VLLM_TARGET_DEVICE</span><span class="o">=</span>cpu<span class="w"> </span>pipx<span class="w"> </span>install<span class="w"> </span>vllm<span class="w"> </span>--verbose
</code></pre></div>

<p>Próbowałem cudów na kiju z tym wyżej, ale ni czorta się nie udało. Brakujące paczki, błędy w kompilacji, próbowałem pipem, pipxem, riserchowałem brakujące zależności, prawie mi się udało z tym co wyżej, ale wciąż kolejne błędy.</p>
<p>Postanowiłem zatem zmienić podejście i skorzystać z pewnego cacka, może ono zadziała - uv to the rescue.</p>
<p>Zainstalowałem zatem vllm za pomocą <code>uv</code> i proces instalacji zakończył się sukcesem, natomiast próba odpalenia modelu już nie, błędy związane z tritonem etc. </p>
<p>Nie zrozumcie mnie źle, to pewnie moja niewiedza czy braki w dokumentacji, ale jeśli ja jako doświadczony programista nie jestem w stanie w ciągu kilku minut wyczytać lub uruchomić appki, postępując zgodnie z krokami w oficjalnych docsach, to ja mówię nie na ten moment.</p>
<p>Llama.cpp it is.</p>
<h3>llama.cpp</h3>
<div class="codehilite"><pre><span></span><code>brew<span class="w"> </span>install<span class="w"> </span>llama.cpp
</code></pre></div>

<p>I już. Tak poszło od buta.</p>
<p>Teraz bierzemy się za samo uruchamianie.</p>
<p>Llama udostępnia kilka trybów uruchomienia, pierwszy z nich to poprzez cli - odpalamy nasz model i w konsoli możemy z nim gadać. My jednak nie chcemy interakcji przez konsole, chcemy mieć endpoint do którego będziemy mogli robić zapytania. Jak to zrobić? Pewnie się trzeba nagimnastykować. Otóż niekoniecznie. Llama.cpp przewiduje taką opcję (vllm też w teorii), pozwalając nam na zwykłe:</p>
<div class="codehilite"><pre><span></span><code>llama-server<span class="w"> </span>-hf<span class="w"> </span>speakleash/Bielik-11B-v2.3-Instruct-GGUF:Q4_K_M
</code></pre></div>

<p>I ciach, gotowe. </p>
<p>Trochę też objaśnienia flaga <code>-hf</code> pozwala nam na zaciągnięcie modelu bezpośrednio z huggingface.co - to takie jakby repozytorium z różnymi modelami. Dzięki temu llama.cpp wie skąd pobrać sobie model. <code>speakleash/Bielik-11B-v2.3-Instruct-GGUF</code> to nazwa modelu <code>Q4_K_M</code> a to zaś taki jakby 'tag', ten konkretny oznacza, że pobieramy model skwantyfikowany do Q4. Yyyyy, co? Możecie o tym myśleć jako o takiej kompresji. To taki jakby skompresowany model, który wymaga mniej resourców do uruchomienia, działa może ODROBINKĘ dosłownie odrobinkę mniej dokładnie, ale za to X razy mniej zasobów żre. Bardzo dobry kompromis. Wracając.</p>
<h2>Serwer gra i buczy</h2>
<p>Na http://localhost:8080 znajdziemy UI a http://localhost:8080/v1/chat/completions to nasz endpoint, kompatybilny z openai, do robienia requestów.</p>
<p>Czy ja wam właśnie chcę powiedzieć, że możecie mieć lokalnie działający polski model językowy za pomocą dwóch komend a w dodatku pójdzie on nawet na zwykłym macu? Tak, a o co chodzi? SLAY. Dobro robotę chłopaki z bielika zrobili. Teraz pora na brawa.</p>
<h3>Użyjmy go</h3>
<p>Aby to zrobić potrzebujemy mieć kilka rzeczy, aby wyglądało to po bożemu.</p>
<p>Pipx - to taka paczka co nam pozwala instalować inne paczki w Pythonie nie brudząc w systemowym Pythonie.</p>
<p>Uv - taki następca poetry, szybszy, lepszy, cool &amp; trendy.</p>
<div class="codehilite"><pre><span></span><code>brew<span class="w"> </span>install<span class="w"> </span>pipx
pipx<span class="w"> </span>install<span class="w"> </span>uv
uv<span class="w"> </span>init<span class="w"> </span>bielik
<span class="nb">cd</span><span class="w"> </span>bielik
uv<span class="w"> </span>add<span class="w"> </span>openai
<span class="nb">source</span><span class="w"> </span>.venv/bin/activate<span class="w"> </span><span class="c1"># aktywujemy naszego venva</span>
</code></pre></div>

<p>Teraz otwórzmy plik <code>main.py</code>.</p>
<p>I tam wklejmy:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">BASE_URL</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;http://localhost:8080/v1&quot;</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="n">BASE_URL</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;...&quot;</span><span class="p">)</span>  <span class="c1"># kluczowy fragment, zmieniamy base_url na nasz lokalny</span>

<span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">():</span>
    <span class="n">completion</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;developer&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Mów niczym Herbert.&quot;</span><span class="p">},</span>
            <span class="p">{</span>
                <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
                <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Jaki jest sens życia?&quot;</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">],</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">completion</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div>

<p>potem</p>
<div class="codehilite"><pre><span></span><code>python<span class="w"> </span>main.py
</code></pre></div>

<blockquote>
<p>Sens życia to temat, który od wieków fascynuje i intryguje ludzi. Z punktu widzenia Herberta, sens życia może być różny dla każdej osoby, ale można wyróżnić pewne wspólne wątki. Oto kilka refleksji na ten temat:</p>
<ol>
<li><strong>Dążenie do prawdy i piękna</strong>:</li>
<li>"Prawda jest jak kamień, który trzeba podnieść i zobaczyć."</li>
<li>
<p>Herbert podkreślał znaczenie poszukiwania prawdy i piękna w życiu. To, co prawdziwe i piękne, ma wartość i nadaje sens naszym działaniom.</p>
</li>
<li>
<p><strong>Ochrona wartości humanistycznych</strong>:</p>
</li>
<li>"Niech wasza mowa będzie tak-tak, nie-nie, a co nadto pochodzi od złego."</li>
<li>
<p>Dla Herberta kluczowe było zachowanie ludzkiej godności i wartości humanistycznych w obliczu nieludzkich warunków.</p>
</li>
<li>
<p><strong>Odpowiedzialność</strong>:</p>
</li>
<li>"Niech wasza mowa będzie prosta, lecz nie prostacka."</li>
<li>
<p>Pisarz podkreślał znaczenie odpowiedzialności za słowa i czyny. Odpowiedzialność jest fundamentem naszych działań i decyzji.</p>
</li>
<li>
<p><strong>Poszukiwanie sensu w codzienności</strong>:</p>
</li>
<li>"Niechaj wasza mowa będzie wolna od obłudy."</li>
<li>
<p>Herbert zachęcał do poszukiwania sensu w codziennych wydarzeniach, w prostych gestach i czynach.</p>
</li>
<li>
<p><strong>Krytyczne myślenie</strong>:</p>
</li>
<li>"Pamiętajcie, że jesteście wyspą."</li>
<li>
<p>Pisarz podkreślał znaczenie krytycznego myślenia, umiejętności patrzenia na świat z dystansem i krytycznego podejścia do otaczającej rzeczywistości.</p>
</li>
<li>
<p><strong>Odkrywanie siebie</strong>:</p>
</li>
<li>"Niechaj wasza mowa będzie niezależna."</li>
<li>
<p>Herbert zachęcał do odkrywania własnej tożsamości, niezależności i autonomii.</p>
</li>
<li>
<p><strong>Pamięć i historia</strong>:</p>
</li>
<li>"Pamiętajcie, że jesteście dziećmi pamięci."</li>
<li>Pisarz podkreślał znaczenie pamięci historycznej i przekazywania jej następnym pokoleniom.</li>
</ol>
<p>Ostatecznie, sens życia dla Herberta leżał w poszukiwaniu prawdy, piękna, odpowiedzialności i niezależności. Był to proces ciągłego rozwoju, który wymagał odwagi, krytycyzmu i refleksji.</p>
</blockquote>
<p>Działa? Działa.</p>
<p>Jestem szczerze zdumiony jak łatwo można obecnie hostować modele lokalnie i z nimi pracować.</p>
<p>AWESOME.</p>
                </div>
            </article>
        </div>
    </section>


<footer class="section">
    <div class="container has-text-centered">
        <p>&copy; <a href="https://grski.pl">Olaf Górski</a> 2025</p>

        <p>Powered by XD philosophy and <a href="https://github.com/grski/braindead">braindead</a>.</p>

    </div>
</footer>
</body>

</html>